{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Try training the n-grams with more training data."
      ],
      "metadata": {
        "id": "nhw3e6ynrknc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "-R-77IOpBpdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import os,random,math,nltk,csv,re,string,gensim\n",
        "from google.colab import drive\n",
        "import pandas as pd, csv\n",
        "from nltk import word_tokenize as tokenize\n",
        "nltk.download('punkt')\n",
        "from scipy import spatial\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "dxG8giVBBoFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "L886U0-swXft"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x77kjg54lwjF"
      },
      "outputs": [],
      "source": [
        "# Mount google drive\n",
        "drive.mount('/content/drive')\n",
        "# Assign the parent directory\n",
        "parentdir = \"/content/drive/My Drive/lab2resources/sentence-completion\"\n",
        "# Assign the training directory\n",
        "trainingdir=os.path.join(parentdir,\"Holmes_Training_Data\")\n",
        "\n",
        "# Function for splitting training and testing data\n",
        "def get_training_testing(training_dir=trainingdir,split=0.5):\n",
        "  filenames=os.listdir(training_dir)\n",
        "  n=len(filenames)\n",
        "  print(\"There are {} files in the training directory: {}\".format(n,training_dir))\n",
        "  #random.seed(53)  #if you want the same random split every time\n",
        "  random.shuffle(filenames)\n",
        "  index=int(n*split)\n",
        "  return(filenames[:index],filenames[index:])\n",
        "\n",
        "training,testing=get_training_testing(trainingdir)\n",
        "\n",
        "%config IPCompleter.greedy=True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "len(os.listdir(trainingdir))"
      ],
      "metadata": {
        "id": "7ccyc47sFwMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yk_VSJRHlwjP"
      },
      "outputs": [],
      "source": [
        "# Read the question and answer files\n",
        "questions=os.path.join(parentdir,\"testing_data.csv\")\n",
        "answers=os.path.join(parentdir,\"test_answer.csv\")\n",
        "\n",
        "# Visualize the questions\n",
        "with open(questions) as instream:\n",
        "    csvreader=csv.reader(instream)\n",
        "    lines=list(csvreader)\n",
        "qs_df=pd.DataFrame(lines[1:],columns=lines[0])\n",
        "qs_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65CYyCGb_Z5B"
      },
      "outputs": [],
      "source": [
        "# Visualize the answers\n",
        "with open(answers) as instream:\n",
        "    csvreader=csv.reader(instream)\n",
        "    lines=list(csvreader)\n",
        "\n",
        "# Store the answers as a list\n",
        "answers_list=[item[1] for item in lines[1:]]\n",
        "# Add to the question dataframe\n",
        "qs_df['answer']=answers_list\n",
        "qs_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N-gram Language Model"
      ],
      "metadata": {
        "id": "RuOL92l0waJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class language_model():\n",
        "  \n",
        "  # Train when initialised\n",
        "  def __init__(self,trainingdir=trainingdir,files=[],discount=0.75):\n",
        "    self.discount=discount\n",
        "    self.training_dir=trainingdir\n",
        "    self.files=files\n",
        "    self.train()\n",
        "  \n",
        "  # Function for training\n",
        "  def train(self):\n",
        "\n",
        "    # Dictionaries for each unigram, bigram, and trigram model\n",
        "    self.unigram={}\n",
        "    self.bigram={}\n",
        "    self.trigram={}\n",
        "\n",
        "    ''' Read the questions and create n-gram dictionaries, make 'unknown' tokens, \n",
        "    apply absolute discounting and convert to probability. '''\n",
        "    self._processfiles()\n",
        "    self._make_unknowns()\n",
        "    self._discount()\n",
        "    self._convert_to_probs()\n",
        "      \n",
        "  # Function for processing a line (question)\n",
        "  def _processline(self,line):\n",
        "\n",
        "    # Tokenise the question and add 'start' and 'end' tokens at each end.\n",
        "    tokens=[\"__START\"]+tokenize(line)+[\"__END\"]\n",
        "    # Previous point for bigram and trigram (w_(n-1))\n",
        "    previous=\"__END\"\n",
        "    # Second previous point for trigram (w_(n-2))\n",
        "    previous2=\"__END\"\n",
        "\n",
        "    # Add counts for each word to the models\n",
        "    for token in tokens:\n",
        "      # Add the unigram count of the word\n",
        "      self.unigram[token]=self.unigram.get(token,0)+1\n",
        "      \n",
        "      # Add the bigram count of the word given its previous word\n",
        "      # {w1:(w2:n)}\n",
        "      current=self.bigram.get(previous,{})\n",
        "      current[token]=current.get(token,0)+1\n",
        "      self.bigram[previous]=current\n",
        "\n",
        "      # Add the trigram count of the word given its two previous words\n",
        "      # An if statement that allows to start from the second word of the line.\n",
        "      # {w1:(w2:(w3:n))}\n",
        "      if previous != \"__END\":\n",
        "        current=self.trigram.get(previous2,{})\n",
        "        current2=current.get(previous,{})\n",
        "        current2[token]=current2.get(token,0)+1\n",
        "        self.trigram[previous2]=current\n",
        "        self.trigram[previous2][previous]=current2\n",
        "\n",
        "      # Move the window to the right\n",
        "      previous2=previous\n",
        "      previous=token\n",
        "\n",
        "  # Function for processing a file\n",
        "  def _processfiles(self):\n",
        "    for afile in self.files:\n",
        "      print(\"Processing {}\".format(afile))\n",
        "      try:\n",
        "        with open(os.path.join(self.training_dir,afile)) as instream:\n",
        "          for line in instream:\n",
        "            line=line.rstrip()\n",
        "            if len(line)>0:\n",
        "              self._processline(line)\n",
        "      except UnicodeDecodeError:\n",
        "        print(\"UnicodeDecodeError processing {}: ignoring rest of file\".format(afile))\n",
        "    \n",
        "  # Function for converting the counts of words to probability distributions       \n",
        "  def _convert_to_probs(self):\n",
        "    \n",
        "    # Get the unigram probability -> count of the word/count of all words\n",
        "    self.unigram={k:v/sum(self.unigram.values()) for (k,v) in self.unigram.items()}\n",
        "    # Get the bigram probability -> count of the previous and current word seen together/count of the previous word\n",
        "    self.bigram={key:{k:v/sum(adict.values()) for (k,v) in adict.items()} for (key,adict) in self.bigram.items()}\n",
        "    self.trigram={key2:{key:{k:v/sum(adict.values()) for (k,v) in adict.items()} for (key,adict) in dicts.items()} for (key2, dicts) in self.trigram.items()}\n",
        "  \n",
        "  # Function for getting the probability of words \n",
        "  def get_prob(self,token,context=\"\",method=\"unigram\"):\n",
        "    \n",
        "    # Get unigram probability of the word, get one of the unknown token if not present.\n",
        "    if method==\"unigram\":\n",
        "      return self.unigram.get(token,self.unigram.get(\"__UNK\",0))\n",
        "\n",
        "    # Get probability of a word and its context word seen together\n",
        "    elif method==\"bigram\":\n",
        "      # Get the bigram probability of the word and context word, get unknown tokens if not present.\n",
        "      bigram=self.bigram.get(context[-1],self.bigram.get(\"__UNK\",{}))\n",
        "      big_p=bigram.get(token,bigram.get(\"__UNK\",0))\n",
        "\n",
        "      # Calculate the reserved probability mass according to the word's unigram probability.\n",
        "      lmbda=bigram[\"__DISCOUNT\"]\n",
        "      uni_p=self.unigram.get(token,self.unigram.get(\"__UNK\",0))\n",
        "\n",
        "      # Calculate the final probability\n",
        "      p=big_p+lmbda*uni_p\n",
        "      return p\n",
        "\n",
        "    # Get probability of a word and its past two words seen together\n",
        "    elif method=='trigram':\n",
        "      trigram=self.trigram.get(context[-2],self.trigram.get(\"__UNK\",{}))\n",
        "      trigram2=trigram.get(context[-1],trigram.get(\"__UNK\",{}))\n",
        "      tri_p=trigram2.get(token,trigram2.get(\"__UNK\",0))\n",
        "      # Calculate the reserved probability\n",
        "      try:\n",
        "        lmbda=trigram2[\"__DISCOUNT\"]\n",
        "      except:\n",
        "        lmbda=self.discount\n",
        "      uni_p=self.unigram.get(token,self.unigram.get(\"__UNK\",0))\n",
        "      p=tri_p+lmbda*uni_p\n",
        "      return p\n",
        "    return 'a'\n",
        "  \n",
        "  # Function for computing the probability of a line and the number of words in the line\n",
        "  def compute_prob_line(self,line,method=\"unigram\"):\n",
        "    \n",
        "    tokens=[\"__START\"]+tokenize(line)+[\"__END\"]\n",
        "    acc=0\n",
        "\n",
        "    # If trigram, starts from the third word of the line.\n",
        "    if method == 'trigram':\n",
        "      for i,token in enumerate(tokens[2:]):\n",
        "        # Get the sum of trigram probabilities of the words in the line\n",
        "        result=self.get_prob(token,tokens[i:i+2],method)\n",
        "        # To prevent math domain error, add the value only if it's non-zero.\n",
        "        if result != 0:\n",
        "          acc+=math.log(result)\n",
        "      return acc,len(tokens[2:])\n",
        "    else:\n",
        "      # If bigram, starts from the second word of the line.\n",
        "      for i,token in enumerate(tokens[1:]):\n",
        "        # Get the sum of bigram probabilities\n",
        "        result=self.get_prob(token,tokens[i:i+1],method)\n",
        "        if result != 0:\n",
        "          acc+=math.log(result)\n",
        "      return acc,len(tokens[1:])\n",
        "\n",
        "  # Function for computing the probability and length of a corpus in a file\n",
        "  def compute_probability(self,filenames=[],method=\"unigram\"):\n",
        "    if filenames==[]:\n",
        "      filenames=self.files\n",
        "    total_p=0\n",
        "    total_N=0\n",
        "    # Process the corpora in the files\n",
        "    for i,afile in enumerate(filenames):\n",
        "      print(\"Processing file {}:{}\".format(i,afile))\n",
        "      try:\n",
        "        with open(os.path.join(self.training_dir,afile)) as instream:\n",
        "          for line in instream:\n",
        "            line=line.rstrip()\n",
        "            if len(line)>0:\n",
        "              p,N=self.compute_prob_line(line,method=method)\n",
        "              total_p+=p\n",
        "              total_N+=N\n",
        "      except UnicodeDecodeError:\n",
        "        print(\"UnicodeDecodeError processing file {}: ignoring rest of file\".format(afile))\n",
        "    return total_p,total_N\n",
        "\n",
        "  # Function for calculating the perplexity of a model\n",
        "  def compute_perplexity(self,filenames=[],method=\"unigram\"):\n",
        "\n",
        "    # Compute the proability and length of the corpus in the given file\n",
        "    p,N=self.compute_probability(filenames=filenames,method=method)\n",
        "    # Apply the formula\n",
        "    pp=math.exp(-p/N)\n",
        "    return pp \n",
        "  \n",
        "  # Function for replacing the words seen less than specific time with an unknown token.\n",
        "  def _make_unknowns(self,known=2):\n",
        "\n",
        "    # Create unknowns for unigram -> when the view count of a word is less than the given parameter, replace it with unknown.\n",
        "    # Increment the view count of unknown if already exists.\n",
        "    for (k,v) in list(self.unigram.items()):\n",
        "      if v<known:\n",
        "        del self.unigram[k]\n",
        "        self.unigram[\"__UNK\"]=self.unigram.get(\"__UNK\",0)+v\n",
        "\n",
        "    # Create unknowns for bigram -> Look at each word in bigram and replace with unknown if not present in the dict.\n",
        "    isknown=0\n",
        "    for (k,adict) in list(self.bigram.items()):\n",
        "      for (kk,v) in list(adict.items()):\n",
        "        isknown=self.unigram.get(kk,0)\n",
        "        # If the second word of the bigram hasn't seen before, replace with unknown and delete the original.\n",
        "        if isknown==0:\n",
        "          adict[\"__UNK\"]=adict.get(\"__UNK\",0)+v\n",
        "          del adict[kk]\n",
        "      isknown=self.unigram.get(k,0)\n",
        "      # If the first word hasn't seen, replace it with unknown and assign the second word to it.\n",
        "      if isknown==0:\n",
        "        del self.bigram[k]\n",
        "        current=self.bigram.get(\"__UNK\",{})\n",
        "        current.update(adict)\n",
        "        self.bigram[\"__UNK\"]=current\n",
        "      # If the word has seen, assign the second word to the word. (Do this in case the second word was replaced to unknown)\n",
        "      else:\n",
        "        self.bigram[k]=adict\n",
        "\n",
        "    # Create unknowns for trigram -> The same process as the bigram but also checks the third word\n",
        "    isknown=0\n",
        "    for (k,dicts) in list(self.trigram.items()):\n",
        "      for (kk,adict) in list(dicts.items()):\n",
        "        for (kkk,v) in list(adict.items()):\n",
        "          isknown=self.unigram.get(kkk,0)\n",
        "          if isknown==0:\n",
        "            adict[\"__UNK\"]=adict.get(\"__UNK\",0)+v\n",
        "            del adict[kkk]\n",
        "\n",
        "        isknown=self.unigram.get(kk,0)\n",
        "        if isknown==0:\n",
        "          del dicts[kk]\n",
        "          current=self.trigram[k].get(\"__UNK\",{})\n",
        "          current.update(adict)\n",
        "          self.trigram[k][\"__UNK\"]=current\n",
        "        else:\n",
        "          self.trigram[k][kk]=adict\n",
        "\n",
        "      isknown=self.unigram.get(k,0)\n",
        "      if isknown==0:\n",
        "        del self.trigram[k]\n",
        "        current=self.trigram.get(\"__UNK\",{})\n",
        "        current.update(dicts)\n",
        "        self.trigram[\"__UNK\"]=current\n",
        "      else:\n",
        "        self.trigram[k]=dicts\n",
        "              \n",
        "  # Function for applying absolute discount\n",
        "  # Discount amount set to 0.75\n",
        "  def _discount(self,discount=0.75):\n",
        "    self.discount=discount\n",
        "    #discount each bigram and trigram count by a small fixed amount\n",
        "    self.bigram={k:{kk:value-discount for (kk,value) in adict.items()} for (k,adict) in self.bigram.items()}\n",
        "    self.trigram={k:{kk:{kkk:value-discount for (kkk,value) in adict.items()} for (kk,adict) in dicts.items()} for (k,dicts) in self.trigram.items()}\n",
        "    \n",
        "    # For each word, store the total amount of the discount so that the total is the same.\n",
        "    # For bigram, just reserve the probability mass according to the first word.\n",
        "    for k in self.bigram.keys():\n",
        "        lamb=len(self.bigram[k])\n",
        "        self.bigram[k][\"__DISCOUNT\"]=lamb*discount\n",
        "\n",
        "    # For trigram, reserve the probability mass according to the second word.\n",
        "    for k in self.trigram.keys():\n",
        "      adict = self.trigram[k]\n",
        "      for kk in adict:\n",
        "        lamb=len(adict[kk])\n",
        "        self.trigram[k][kk][\"__DISCOUNT\"]=lamb*discount"
      ],
      "metadata": {
        "id": "0cyiZAiioLp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create filesets with the training and testing data\n",
        "MAX_FILES=100\n",
        "filesets={\"training\":training[:MAX_FILES],\"testing\":testing[:MAX_FILES]}\n",
        "\n",
        "# Train the language model with the three methods\n",
        "mylm=language_model(files=filesets[\"training\"])\n",
        "methods=[\"unigram\",\"bigram\",'trigram']\n",
        "\n",
        "# Compute perplexity of the training and testing data using each model and store the results for plotting\n",
        "perplexities={}\n",
        "for f,names in list(filesets.items()):\n",
        "  perplexities[f]={}\n",
        "  for m in methods:\n",
        "    p=mylm.compute_perplexity(filenames=names,method=m)\n",
        "    print(\"Perplexity on {} with {} method is {}\".format(f,m,p))\n",
        "    perplexities[f][m]=p"
      ],
      "metadata": {
        "id": "5Z3sUIs5pmyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The function for plotting perplexity on a bar chart\n",
        "def autolabel(rects):\n",
        "  for rect in rects:\n",
        "    height=rect.get_height()\n",
        "    # Decide the location where the value for each bar is presented \n",
        "    location=height\n",
        "    # If the value is negative, place the value right above the x-axis\n",
        "    if height<0:\n",
        "      location=0\n",
        "    # Annotate the value for each bar\n",
        "    ax.annotate('{:.2f}'.format(height),\n",
        "                xy=(rect.get_x() + rect.get_width() / 2, location),\n",
        "                xytext=(0, 8),  # 3 points vertical offset\n",
        "                textcoords=\"offset points\",\n",
        "                ha='center', va='bottom')"
      ],
      "metadata": {
        "id": "_fTW4o1amzuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the results\n",
        "labels=['1 month', '6 months', '1 year']\n",
        "\n",
        "x = np.arange(len(labels))  # the label locations\n",
        "width=0.4  # the width of the bars\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "rects1=ax.bar(x - width/2, perplexities['training'].values(), width, label='training')\n",
        "rects2=ax.bar(x + width/2, perplexities['testing'].values(), width, label='testing')\n",
        "ax.set_ylabel('Perplexity')\n",
        "ax.set_title('The perplexity of each model on training and testing data with 100 training documents')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(methods)\n",
        "ax.set_ylim((0,800))\n",
        "ax.legend()\n",
        "\n",
        "autolabel(rects1)\n",
        "autolabel(rects2)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fhitaUoNndEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class for word similarity method using word embeddings\n",
        "class word_embedding:\n",
        "\n",
        "  # Load pre-trained word2vec\n",
        "  def __init__(self):\n",
        "    embedding_file='/content/drive/MyDrive/GoogleNews-vectors-negative300.bin'\n",
        "    self.embeddings=gensim.models.KeyedVectors.load_word2vec_format(embedding_file, binary=True)\n",
        "\n",
        "  def word2vec(self,tokens):\n",
        "    # Define the dimensionality (300 dims)\n",
        "    dim=self.embeddings['word'].size\n",
        "\n",
        "    # Make a list of embeddings of each word\n",
        "    word_vec=[]\n",
        "    for word in tokens:\n",
        "      if word in self.embeddings:\n",
        "        word_vec.append(self.embeddings[word])\n",
        "      # If the word is not present in the embedding, assign a random vector\n",
        "      else:\n",
        "        word_vec.append(np.random.uniform(-0.25,0.25,dim))\n",
        "    return word_vec\n",
        "\n",
        "  # Function for calculating the similarity of the question vec and target vec.\n",
        "  def total_similarity(self,vec,ques_vec):\n",
        "    score = 0\n",
        "    # Compare all the word vectors in the question to the target vector in terms of cosine similarity\n",
        "    # Increase of decrease score according to the result\n",
        "    # Lower cosine distance -> More similar -> Higher score\n",
        "    for v in ques_vec:\n",
        "      score += (1 - spatial.distance.cosine(vec, v))\n",
        "    return score"
      ],
      "metadata": {
        "id": "9K5HFsCxAiTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an embedding object\n",
        "emb = word_embedding()"
      ],
      "metadata": {
        "id": "A47dRQudvkRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEM7o_HXlwjZ"
      },
      "outputs": [],
      "source": [
        "# Class for reading and processing a question\n",
        "class question:\n",
        "\n",
        "  def __init__(self,aline):\n",
        "      self.fields=aline\n",
        "\n",
        "  # Get the given field from the question\n",
        "  def get_field(self,field):\n",
        "    return self.fields[question.colnames[field]]\n",
        "\n",
        "  # Get the tokenized question with 'start' and 'end' tokens at each end.\n",
        "  def get_tokens(self):\n",
        "    return [\"__START\"]+tokenize(self.fields[question.colnames[\"question\"]])+[\"__END\"]\n",
        "\n",
        "  # Add the answer field.\n",
        "  def add_answer(self,fields):\n",
        "      self.answer=fields[1]\n",
        "\n",
        "  # Get window amount of left context words of the blank.\n",
        "  def get_left_context(self,window=1,target=\"_____\"):\n",
        "    found=-1\n",
        "    sent_tokens=self.get_tokens()\n",
        "    for i,token in enumerate(sent_tokens):\n",
        "      if token==target:\n",
        "        found=i\n",
        "        break  \n",
        "    if found>-1:\n",
        "      return sent_tokens[found-window:found]\n",
        "    else:\n",
        "      return []\n",
        "\n",
        "  # Get window amount of the right context words of the blank.\n",
        "  def get_right_context(self,window=1,target=\"_____\"):\n",
        "    found=-1\n",
        "    sent_tokens=self.get_tokens()\n",
        "    for i,token in enumerate(sent_tokens):\n",
        "      if token==target:\n",
        "        found=i\n",
        "        break  \n",
        "    if found>-1:\n",
        "      return sent_tokens[found+1:found+window+1]\n",
        "    else:\n",
        "      return []\n",
        "\n",
        "  # Make a prediction for the question using a given method\n",
        "  def predict(self,method='word_embedding_total',model=mylm):\n",
        "    if method==\"bigram_backoff\":\n",
        "        return self.choose_backoff(model,methods=[\"unigram\",\"bigram\"],i=1)\n",
        "    elif method=='trigram_backoff':\n",
        "      return self.choose_backoff(model,methods=['unigram','bigram','trigram'],i=2)\n",
        "    elif method=='word_embedding_total':\n",
        "      return self.embedding_prediction(emb=emb,method='total')\n",
        "    elif method=='word_embedding_partial':\n",
        "      return self.embedding_prediction(emb=emb,method='partial')\n",
        "    else:\n",
        "      return self.choose(model,method=method)\n",
        "\n",
        "  # Function for predicting using word2vec similarity\n",
        "  def embedding_prediction(self,emb,method):\n",
        "    choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
        "    # Tokenize the question (not use the class method since it does not need start and end tokens)\n",
        "    \n",
        "    # Create a list of vectors consisting of the question's word vectors\n",
        "    scores =[]\n",
        "    candidates = [self.get_field(ch+\")\") for ch in choices]\n",
        "    cand_vec = emb.word2vec(candidates)\n",
        "    \n",
        "    # If the method is 'total' which compares the choice word vector to all the words in the question\n",
        "    if method=='total':\n",
        "      tokens = tokenize(self.fields[question.colnames[\"question\"]])\n",
        "      ques_vec = emb.word2vec(tokens=tokens)\n",
        "    # If the method is not total, only compare the choice word to the context words returned with the given window \n",
        "    else:\n",
        "      tokens = self.get_left_context(window=1) + self.get_right_context(window=1)\n",
        "      ques_vec = emb.word2vec(tokens=tokens)\n",
        "\n",
        "    # Calculate total similarity of all the choice words\n",
        "    for word in cand_vec:\n",
        "      s = emb.total_similarity(word, ques_vec)        \n",
        "      scores.append(s)\n",
        "    # Return one with the highest score\n",
        "    idx = scores.index(max(scores))\n",
        "    ans = choices[idx]\n",
        "    return ans\n",
        "      \n",
        "  # Make predictions and get scores by comparing to the correct answers\n",
        "  def predict_and_score(self,method=\"word_embedding_total\",model=mylm):\n",
        "    prediction=self.predict(method=method,model=model)\n",
        "    if prediction ==self.answer:\n",
        "      return 1\n",
        "    else:\n",
        "      return 0\n",
        "\n",
        "  # Calculate probabilities of choice words using the given method and return one with the highest probability.\n",
        "  def choose(self,lm,method=\"trigram\",choices=[]):\n",
        "    if choices==[]:\n",
        "      choices=[\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
        "\n",
        "    # For unigram, simply get the probability of the choice word.\n",
        "    if method=='unigram':\n",
        "      probs=[lm.unigram.get(self.get_field(ch+\")\"),0) for ch in choices]\n",
        "\n",
        "    # For bigram, get a context word from the left and right each and calculate probabilities of the word given each context word.\n",
        "    # Then multiply both probabilties\n",
        "    elif method==\"bigram\":\n",
        "      rc=self.get_right_context(window=1)\n",
        "      lc=self.get_left_context(window=1)\n",
        "      probs=[lm.get_prob(rc[0],self.get_field(ch+\")\"),method)*\n",
        "             lm.get_prob(self.get_field(ch+\")\"),lc,method) for ch in choices]\n",
        "\n",
        "    # For trigram, get two context words from the left and right and calculate trigram probabilities.\n",
        "    else:\n",
        "      rc=self.get_right_context(window=2)\n",
        "      lc=self.get_left_context(window=2)\n",
        "      probs=[lm.get_prob(rc[1],[self.get_field(ch+\")\"),rc[0]],method)*\n",
        "             lm.get_prob(self.get_field(ch+\")\"),lc,method) for ch in choices]\n",
        "\n",
        "    # Return one with the highest probability\n",
        "    maxprob=max(probs)\n",
        "    bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "\n",
        "    # If there are multiple options left, choose one randomly.\n",
        "    if len(bestchoices)>1:\n",
        "       print(\"Randomly choosing from {}\".format(len(bestchoices)))\n",
        "    return np.random.choice(bestchoices)\n",
        "\n",
        "\n",
        "  # Instead of choosing randomly when there are multiple options, back off to a lower order n-gram.\n",
        "  # Recursive and controlled with parameter 'i'\n",
        "  # Starts with the method of the last element of the methods parameter, and back off to the second last one and so on.\n",
        "  # Higher order methods should be positioned later.\n",
        "  def choose_backoff(self,lm,methods=['unigram','bigram','trigram'],choices=[\"a\",\"b\",\"c\",\"d\",\"e\"],i=2):\n",
        "\n",
        "    # Base case: return an answer according to the unigram probability if it couln't be sorted out with bigram model.\n",
        "    if methods[i] == 'unigram':\n",
        "      return self.choose(lm,'unigram',choices)\n",
        "\n",
        "    # Get bigram probabilities\n",
        "    elif methods[i] == 'bigram':\n",
        "      rc=self.get_right_context(window=1)\n",
        "      lc=self.get_left_context(window=1)\n",
        "      probs=[lm.get_prob(rc[0],self.get_field(ch+\")\"),'bigram')*\n",
        "             lm.get_prob(self.get_field(ch+\")\"),lc,'bigram') for ch in choices]\n",
        "\n",
        "    # Get trigram probabilities\n",
        "    elif methods[i] == 'trigram':\n",
        "      rc=self.get_right_context(window=2)\n",
        "      lc=self.get_left_context(window=2)\n",
        "      probs=[lm.get_prob(rc[1],[self.get_field(ch+\")\"),rc[0]],'trigram')*\n",
        "            lm.get_prob(self.get_field(ch+\")\"),lc,'trigram') for ch in choices]\n",
        "\n",
        "    # Get the best choices and if there are multiple, call this function recursively with the lower order model.\n",
        "    maxprob=max(probs)\n",
        "    bestchoices=[ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
        "    if len(bestchoices)>1:\n",
        "      print(\"Backing off on {} and window {}\".format(len(bestchoices),i))\n",
        "      self.choose_backoff(lm,methods[:i],bestchoices,i-1)\n",
        "    \n",
        "    # There will be only one element in bestchoices\n",
        "    return bestchoices[0]\n",
        "\n",
        "# Class for processing the questions in files.\n",
        "class scc_reader:\n",
        "    \n",
        "  def __init__(self,qs=questions,ans=answers):\n",
        "    self.qs=qs\n",
        "    self.ans=ans\n",
        "    self.read_files()\n",
        "      \n",
        "  def read_files(self):\n",
        "    # read in the question file\n",
        "    with open(self.qs) as instream:\n",
        "      csvreader=csv.reader(instream)\n",
        "      qlines=list(csvreader)\n",
        "    \n",
        "    # store the column names as a reverse index so they can be used to reference parts of the question\n",
        "    question.colnames={item:i for i,item in enumerate(qlines[0])}\n",
        "    \n",
        "    # create a question instance for each line of the file (other than heading line)\n",
        "    self.questions=[question(qline) for qline in qlines[1:]]\n",
        "    \n",
        "    # read in the answer file\n",
        "    with open(self.ans) as instream:\n",
        "      csvreader=csv.reader(instream)\n",
        "      alines=list(csvreader)\n",
        "        \n",
        "    # add answers to questions so predictions can be checked    \n",
        "    for q,aline in zip(self.questions,alines[1:]):\n",
        "      q.add_answer(aline)\n",
        "      \n",
        "  def get_field(self,field):\n",
        "    return [q.get_field(field) for q in self.questions] \n",
        "  \n",
        "  def predict(self,method=\"word_embedding_total\"):        \n",
        "    return [q.predict(method=method) for q in self.questions]\n",
        "    \n",
        "  def predict_and_score(self,method=\"word_embedding_total\",model=mylm):\n",
        "    scores=[q.predict_and_score(method=method,model=model) for q in self.questions]\n",
        "    return sum(scores)/len(scores)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeOfYNyjlwjT"
      },
      "outputs": [],
      "source": [
        "SCC = scc_reader()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGT-rozmIudM"
      },
      "outputs": [],
      "source": [
        "# Train with different numbers of training documents\n",
        "numbers=[20, 50, 100]\n",
        "lms = {}\n",
        "for n in numbers:\n",
        "  mylm=language_model(trainingdir=trainingdir,files=training[:n],discount=0.3)\n",
        "  lms[n]=mylm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results={}\n",
        "methods=['trigram_backoff','trigram','bigram','bigram_backoff','unigram']\n",
        "for (n, model) in lms.items():\n",
        "  results[n]={}\n",
        "  for method in methods:\n",
        "    acc=SCC.predict_and_score(method,model)\n",
        "    results[n][method]=acc\n",
        "\n",
        "results_pd=pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "hkpg9Ywc9itO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the perplexity of each model\n",
        "labels = ['unigram','bigram','trigram']\n",
        "\n",
        "x = np.arange(len(labels))  # the label locations\n",
        "width=0.4  # the width of the bars\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "rects1=ax.bar(x - width/2, perplexities['training'].values(), width, label='training')\n",
        "rects2=ax.bar(x + width/2, perplexities['testing'].values(), width, label='testing')\n",
        "ax.set_ylabel('Perplexity')\n",
        "ax.set_title('The perplexity of each model on training and testing data with 100 training documents')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(methods)\n",
        "ax.set_ylim((0,800))\n",
        "ax.legend()\n",
        "\n",
        "autolabel(rects1)\n",
        "autolabel(rects2)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ixFe4coKMTPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubgbfMCOlwjU"
      },
      "outputs": [],
      "source": [
        "# Calculate accuracy of all the models on the challenge\n",
        "tri_backoff_sc = SCC.predict_and_score('trigram_backoff')\n",
        "trigram_sc = SCC.predict_and_score('trigram')\n",
        "bigram_sc = SCC.predict_and_score('bigram')\n",
        "bi_backoff_sc=SCC.predict_and_score('bigram_backoff')\n",
        "unigram_sc=SCC.predict_and_score('unigram')\n",
        "embedding_total_sc=SCC.predict_and_score('word_embedding_total')\n",
        "embedding_partial_sc=SCC.predict_and_score('word_embedding_partial')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_partial_sc_1=SCC.predict_and_score('word_embedding_partial') # compare only a single word from the left and right\n",
        "print(embedding_partial_sc_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_34CAmmel03b",
        "outputId": "944471db-90b3-43ad-fb8a-10cd469e6bf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.2778846153846154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_partial_sc_2=SCC.predict_and_score('word_embedding_partial') # compare two words each form the left and right\n",
        "print(embedding_partial_sc_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rLL7t0zl9tU",
        "outputId": "0774e185-e8c9-4981-e9ee-42bfbf680010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.32019230769230766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_total_sc=SCC.predict_and_score('word_embedding_total') # compare all the words\n",
        "print(embedding_total_sc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BR5nm7Afl9zZ",
        "outputId": "27724824-5d06-453d-a13e-d71e7c27909d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.35865384615384616\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_results=[embedding_partial_sc_1,embedding_partial_sc_2,embedding_total_sc]"
      ],
      "metadata": {
        "id": "tjM9rTxxme9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the accuracy of the distributional similarity method\n",
        "labels=['First Method', 'Second Method', 'Third Method']\n",
        "\n",
        "x = np.arange(len(labels))  # the label locations\n",
        "width=0.4  # the width of the bars\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "rects1=ax.bar(x, embedding_results, width)\n",
        "ax.set_ylabel('Accuracy')\n",
        "ax.set_title('The Accuracy of Each Method of Distributional Similarity')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels)\n",
        "ax.set_ylim((0,1))\n",
        "\n",
        "autolabel(rects1)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ylva_WmulkU4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "week3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}